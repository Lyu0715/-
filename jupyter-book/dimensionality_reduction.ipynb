{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4aaa03a",
   "metadata": {},
   "source": [
    "(ch:dimensionalityReduction)=\n",
    "# 차원축소"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d349edf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**감사의 글**\n",
    "\n",
    "자료를 공개한 저자 오렐리앙 제롱과 강의자료를 지원한 한빛아카데미에게 진심어린 감사를 전합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1888a1",
   "metadata": {},
   "source": [
    "**소스코드**\n",
    "\n",
    "본문 내용의 일부를 파이썬으로 구현한 내용은 \n",
    "[(구글코랩) 차원축소](https://colab.research.google.com/github/codingalzi/handson-ml3/blob/master/notebooks/code_dimensionality_reduction.ipynb)에서 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc790b01",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**주요 내용**\n",
    "\n",
    "샘플의 특성이 너무 많으면 학습이 매우 느리거나 어려워지는 현상를\n",
    "**차원의 저주**라 한다.\n",
    "이 문제를 해결하기 위해 특성 수를 (크게) 줄여서 학습 불가능한 문제를 학습 가능한 문제로 만드는\n",
    "**차원축소** 기법을 사용할 수 있다.\n",
    "차원축소로 인한 정보손실을 어느 정도 감안하면서 훈련 속도와 성능을 최대로 유지하는 것을\n",
    "목표로 삼는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54367d4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "예를 들어, MNIST 데이터셋의 경우 사진의 중앙에만 집중하거나({prf:ref}`exp-MNIST-feature-importance`)\n",
    "인접한 픽셀의 평균값만을 이용해도 숫자 인식에 별 문제 없다.\n",
    "주성분 분석(PCA) 기법을 이용하여 손글씨 사진의 784개 픽셀 대신 154개만 대상으로 삼아도\n",
    "충분히 학습이 가능함을 보일 것이다.\n",
    "\n",
    "차원축소 기법은 또한 데이터 시각화에도 활용된다.\n",
    "데이터의 차원(특성 수)을 2, 3차원으로 줄이면 데이터셋을 시각화할 수 있다.\n",
    "데이터 시각화는 데이터 군집 같은 시각적인 패턴을 감지하여 데이터에 대한 통찰을 얻거나\n",
    "데이터에 대한 정보를 제3자에게 전달하는 데에 도움된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2686db52",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "차원축소를 위한 접근법은 크게 사영 기법과 다양체 학습 기법으로 나뉜다. \n",
    "사영 기법 알고리즘으로 PCA(주성분 분석)와 임의 사영<font size='2'>Random Projection</font>을,\n",
    "다양체 학습 알고리즘으로 LLE(국소적 선형 임베딩)을 소개한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41974d2b",
   "metadata": {
    "colab_type": "text",
    "id": "l-Y7yrhc7cM6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 차원의 저주"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a4cacc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3차원을 초과하는 고차원의 공간을 상상하는 하는 일은 매우 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748019eb",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch08/homl08-01.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4efa670",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "차원의 커질 수록 두 지점 사이의 거리가 매우 멀어진다.\n",
    "이는 특성 수가 아주 많은 경우, 훈련 샘플 사이의 거리가 매우 커서 과대적합 위험도가 커짐을 의미한다.\n",
    "이유는 두 샘플 사이의 거리가 멀어서 기존 값들을 이용한 추정이 여러 과정을 거쳐야 하기 때문에\n",
    "훈련셋에 과학게 의존하게 되기 때문이다.\n",
    "\n",
    "훈련셋의 크기를 키워야 하지만 고차원의 경우 충분히 많은 샘플 수를 준비하는 일은 사실상 불가능하다는 것이\n",
    "**차원의 저주**라는 표현의 핵심이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5027308",
   "metadata": {
    "colab_type": "text",
    "id": "dsT96srL9uAE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 차원축소 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2927aa3",
   "metadata": {
    "colab_type": "text",
    "id": "dsT96srL9uAE",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "훈련 샘플이 고차원 공간의 일부인 저차원 부분공간에 가깝게 놓여 있는 경우가 일반적으로 발생한다.\n",
    "이런 경우 고차원의 데이터셋을 저차원의 데이터셋으로 변환시켜도 정보의 손실이 크지 않게 유지할 수 있다.\n",
    "이것이 차원축소 기법이며 크게 사영 기법과 다양체 학습 기법으로 나뉜다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e2b210",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 사영 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf423f",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* $n$차원 공간에 존재하는 $d$차원 부분공간을 $d$차원 공간으로 사영하기. 단, $d < n$.\n",
    "\n",
    "* 예제\n",
    "    * 왼쪽 3차원에 존재하는 적절한 2차원 평면으로 사영하면 적절한 2차원 상의 이미지를 얻게됨.\n",
    "    * 오른쪽 2차원 이미지에 사용된 축 $z_1$과 $z_2$를 적절하게 찾는 게 주요 과제임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28a11a6",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch08/homl08-02-1.png\" width=\"400\"/> </td>\n",
    "        <td></td>\n",
    "        <td> <img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch08/homl08-02-2.png\" width=\"400\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb23ca7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**부적절한 사영**\n",
    "\n",
    "* 사영이 경우에 따라 보다 복잡한 결과를 낼 수 있음.\n",
    "\n",
    "* 롤케이크를 $x_1$과 $x_2$ 축으로 사영하면 샘플 구분이 보다 어려워짐."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8952e3cd",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch08/homl08-04.png\" width=\"300\"/> </td>\n",
    "        <td></td>\n",
    "        <td> <img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch08/homl08-06.png\" width=\"320\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee49517d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 다양체 학습 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f94e750",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**다양체**\n",
    "\n",
    "고차원 공간에서 저차원 공간을 접거가 접거나 비틀어서 생성한 공간을 \n",
    "**다양체**<font size='2'>manifold</font>라 부른다.\n",
    "예를 들어, 롤케이크<font size='2'>Swiss roll</font>는 \n",
    "2차원 평면을 돌돌 말아 3차원 공간상에 존재하는 2D 다양체다. \n",
    "실제로 롤케이크을 조심해서 펴면 보다 적절한 2차원 공간으로 변환된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe866a23",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch08/homl08-07.png\" width=\"310\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461105bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**다양체 가설**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e0fdb2",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "롤케이크와 같은 다양체의 경우 사영 보다는 접히거나 비틀어진 것을 잘 펼치면 \n",
    "보다 적절한 2차원 공간으로 변환된다.\n",
    "이처럼 숨겨진 다양체를 찾는 과정이 **다양체 학습**<font size='2'>Manifold Learning</font>이다. \n",
    "다양체 학습은 대부분의 고차원 데이터셋이 더 낮은 차원의 다양체에 가깝다는가설에 근거한다.\n",
    "아래 그램의 위쪽 데이터셋의 경우가 그렇다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c94e28",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "다양체 가설은 또한 저차원의 다양체 공간으로 차원축소를 진행하면 보다 간단한 다양체가 된다라는 \n",
    "가설과 함께 사용된다. \n",
    "하지만 이는 경우에 따라 다르다.\n",
    "아래 그림의 아랫쪽 데이터셋의 경우는 차원축소를 진행하면 데이터셋이 보다 복잡해진다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7667d8",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch08/homl08-08.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b470d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA(주성분 분석)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6841ee93",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 훈련 데이터에 가장 가까운 초평면(hyperplane)을 정의한 다음, 그 평면에 사영하는 기법\n",
    "\n",
    "* **주성분 분석**<font size='2'>principal component analysis</font>(PCA)이 핵심.\n",
    "\n",
    "* 분산 보존 개념과 주성분 개념이 중요함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fbd6dd",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### 분산 보존"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba2bd89",
   "metadata": {},
   "source": [
    "* 분산 보존: 저차원으로 사영할 때 훈련 세트의 분산이 최대한 유지되도록 축을 지정해야 함.\n",
    "\n",
    "* 예제: 아래 그림에서 $c_1$ 벡터가 위치한 실선 축으로 사영하는 경우가 분산을 최대한 보존함.\n",
    "    그러면 $c_1$에 수직이면서 분산을 최대로 보존하는 축은 $c_2$임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ada1f2e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch08/homl08-09.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39e57be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**주성분**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e608f",
   "metadata": {},
   "source": [
    "* 첫째 주성분: 분산을 최대한 보존하는 축\n",
    "\n",
    "* 둘째 주성분: 첫째 주성분과 수직을 이루면서 분산을 최대한 보존하는 축\n",
    "\n",
    "* 셋째 주성분: 첫째, 둘째 주성분과 수직을 이루면서 분산을 최대한 보존하는 축\n",
    "\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1c677e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**주성분과 사영**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d5230f",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 특잇값 분해(SVD) 기법을 이용하면 쉽게 해결됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10813dd3",
   "metadata": {},
   "source": [
    "* 특잇값 분해: m x n 모양을 가지며, 평균값이 0인 데이터셋 $X$가 주어졌을 때 \n",
    "    아래 조건을 만족시키는 세 개의 행렬 \n",
    "    $U$, $\\Sigma$, $V$가 존재.\n",
    "    - $U$: m x m 행렬\n",
    "    - $\\Sigma$: m x n 모양의 대각행렬(diagonal matrix). \n",
    "    - $V$: n x n 행렬. 윗첨자 $T$는 전치행렬을 의미함.\n",
    "\n",
    "    $$\n",
    "    X = U\\, \\Sigma \\, V^{\\!T}\n",
    "    $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc35cd66",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 주성분 벡터는 행렬 $V$의 열에 해당하며, 따라서\n",
    "    $d$차원으로의 사영은 아래와 같이 계산됨:\n",
    "    \n",
    "    $$\n",
    "    X\\, (V\\text{[: ,  :d]})\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aacc6ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**사이킷런의 `PCA` 모델**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fb292e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 사이킷런의 PCA 모델 제공\n",
    "    * SVD 기법 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eb4ff4",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 예제: 데이터셋의 차원을 2로 줄이기\n",
    "\n",
    "    ```python\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    pca = PCA(n_components = 2)\n",
    "    X2D = pca.fit_transform(X)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dba3b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 설명 분산 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee466d44",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* `explained_variance_ration_` 속성 변수: 각 주성분에 대한 원 데이터셋의 분산 비율 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87119870",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 예제: 아래 사영 그림에서 설명된 3차원 데이터셋의 경우.\n",
    "    * $z_1$ 축: 75.8%\n",
    "    * $z_2$ 축: 15.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0c2a80",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> pca.explained_variance_ratio_\n",
    "array([0.7578477 , 0.15186921])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ca127",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch08/homl08-02-1.png\" width=\"400\"/> </td>\n",
    "        <td></td>\n",
    "        <td> <img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch08/homl08-02-2.png\" width=\"400\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517e9535",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**적절한 차원**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4257e9eb",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 적절한 차원: 밝혀진 분산 비율의 합이 95% 정도 되도록 하는 주성분들로 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46100bc4",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 데이터 시각화 목적의 경우: 2개 또는 3개"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f3f2f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**설명 분산 비율 활용**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda2500",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 설명 분산 비율의 합과 차원 사이의 그래프 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2ec1c1",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 설명 분산의 비율의 합의 증가가 완만하게 변하는 지점(elbow)에 주시할 것."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f1380",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch08/homl08-10.png\" width=\"400\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb0a3eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**(MNIST 활용 예제) 압축을 위한 PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dba867",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* PCA를 MNIST 데이터셋의 차원축소를 위해 사용할 수 있음.\n",
    "\n",
    "* MINST 데이터셋의 주성분 분석을 통해 95% 정도의 분산을 유지하려면 154개 정도의 주성분만 사용해도 됨.\n",
    "\n",
    "* 아래 코드: 154개 주성분 사용하여 차원축소하기\n",
    "\n",
    "    ```python\n",
    "    pca = PCA(n_components = 154)\n",
    "    X_reduced = pca.fit_transform(X_train)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b15e8a4",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### 재구성 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f595d889",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 차원축소 결과:\n",
    "    * 784차원을 154 차원으로 줄임.\n",
    "    * 유실된 정보: 5%\n",
    "    * 크기: 원본 데이터셋 크기의 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907848d1",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 원본과의 비교: 정보손실 크지 않음 확인 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45c40dd",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch08/homl08-11.png\" width=\"400\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b5648b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 랜덤 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db503868",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 주성분 선택을 위해 사용되는 SVD 알고리즘을 확률적으로 작동하도록 만드는 기법\n",
    "\n",
    "* 보다 빠르게 지정된 개수의 주성분에 대한 근삿값을 찾아줌.\n",
    "\n",
    "* $d$가 $n$ 보다 많이 작으면 기본 SVD 보다 훨씬 빠름.\n",
    "    - 기존의 특잇값 분해 알고리즘의 시간 복잡도: $O(m \\times n^2) + O(n^3)$\n",
    "    - 랜덤 특잇값 분해 알고리즘의 시간 복잡도: $O(m \\times d^2) + O(d^3)$\n",
    "\n",
    "\n",
    "* 아래 코드: `svd_solver` 옵션을 `\"randomized\"`로 설정\n",
    "\n",
    "    ```python\n",
    "    rnd_pca = PCA(n_components = 154, svd_solver=\"randomized\")\n",
    "    X_reduced = rnd_pca.fit_transform(X_train)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a3f81",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 점진적 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22189bcc",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 훈련세트를 미니배치로 나눈 후 IPCA(점진적 PCA)에 하나씩 주입 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438aad01",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 온라인 학습에 적용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d01775",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* `partial_fit()` 활용에 주의할 것.\n",
    "\n",
    "    ```python\n",
    "    from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "    n_batches = 100\n",
    "    inc_pca = IncrementalPCA(n_components=154)\n",
    "    for X_batch in np.array_split(X_train, n_batches):\n",
    "        inc_pca.partial_fit(X_batch)\n",
    "\n",
    "    X_reduced = inc_pca.transform(X_train)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e11b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**넘파이의 `memmap()` 클래스 활용**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bdd56f",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 바이너리 파일로 저장된 (매우 큰) 데이터셋을 마치 메모리에 들어있는 것처럼 취급할 수 있는 도구 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca542d9",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 이를 이용하여 미니배치/온라인 학습 가능\n",
    "\n",
    "    ```python\n",
    "    X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\n",
    "    inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "    inc_pca.fit(X_mm)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4922be",
   "metadata": {},
   "source": [
    "## 임의 사영"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b011fc6",
   "metadata": {},
   "source": [
    "**존슨-린덴슈트라우스 정리**\n",
    "\n",
    "존슨-린덴슈트라우스<font size='2'>Johnson-Lindenstrauss</font> 정리에 의해 \n",
    "고차원의 데이터를 적절한 크기의 저차원으로 임의적으로 사영하더라도\n",
    "데이터셋의 정보를 많이 잃어버리지 않는다.\n",
    "적절한 크기의 차원 $d$는 정보를 얼마나 잃어도 되는가에 따라 결정되며,\n",
    "아래 값을 만족하면 된다.\n",
    "단, $m$ 은 훈련셋의 크기를 나타내며, \n",
    "$\\varepsilon$ 은 정보손실 정도를 가리킨다.\n",
    "\n",
    "$$\n",
    "d \\ge \\frac{4 \\log(m)}{\\frac{1}{2} \\varepsilon^2 - \\frac{1}{3} \\varepsilon^3}\n",
    "$$\n",
    "\n",
    "**임의 사영**<font size='2'>Random Projection</font>은 존슨-린덴슈트라우스 정리를\n",
    "이용한 사영을 가리킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc72861",
   "metadata": {},
   "source": [
    ":::{admonition} $\\varepsilon$ 의 역할\n",
    ":class: info\n",
    "\n",
    "$\\varepsilon$ 은 사영된 두 데이터 사이의 거리가 기존의 거리에 비해 차이날 수 있는 정도이다. \n",
    "예를 들어 $\\varepsilon=0.1$ 로 지정하면 \n",
    "기존의 두 데이터의 거리의 제곱에 비해 사영된 두 데이터 사이의 거리의 제곱이 10% 정도의 차이만\n",
    "허용한다는 의미다.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f294c1",
   "metadata": {},
   "source": [
    "**사이키런의 `GaussianRandomProjection` 모델**\n",
    "\n",
    "`GaussianRandomProjection` 모델이 앞서 존슨-린덴슈트라우스 정리를 이용한 \n",
    "임의 사영을 실행한다. \n",
    "\n",
    "```python\n",
    "gaussian_rnd_proj = GaussianRandomProjection(eps=ε, random_state=42)\n",
    "X_reduced = gaussian_rnd_proj.fit_transform(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda0dcc3",
   "metadata": {},
   "source": [
    "**사이키런의 `SparseRandomProjection` 모델**\n",
    "\n",
    "희소 행렬을 사용하는 `GaussianRandomProjection` 모델이며 보다 빠르고 메모리 효율적이다. \n",
    "\n",
    "```python\n",
    "gaussian_rnd_proj = SparseRandomProjection(eps=ε, random_state=42)\n",
    "X_reduced = gaussian_rnd_proj.fit_transform(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c6bf3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LLE(국소적 선형 임베딩)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f7041b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**아이디어**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33568cb9",
   "metadata": {},
   "source": [
    "* 대표적인 다양체 학습 기법\n",
    "\n",
    "* 롤케이크 데이터셋의 경우처럼 전체적으론 비선형인 다양체이지만 국소적으로는 데이터가 선형적으로 연관되어 있음.\n",
    "\n",
    "* 국소적 관계가 가장 잘 보존되는 훈련 세트의 저차원 표현 찾을 수 있음.\n",
    "\n",
    "* 사영이 아닌 다양체 학습에 의존"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0ea47b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**예제: 롤케이크**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b863e76",
   "metadata": {},
   "source": [
    "```python\n",
    "X_swiss, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
    "X_unrolled = lle.fit_transform(X_swiss)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71661bf",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch08/homl08-04.png\" width=\"300\"/> </td>\n",
    "        <td></td>\n",
    "        <td> <img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch08/homl08-14.png\" width=\"370\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621da6ba",
   "metadata": {},
   "source": [
    "## 부록: 사이킷런의 기타 차원 축소 모델\n",
    "\n",
    "* 다차원 스케일링<font size='2'>Multidimensional Scaling</font>(MDS)\n",
    "* Isomap\n",
    "* t-SNE(t-Distributed Stochasting Neighbor Embedding)\n",
    "* 선형 판별 분석<font size='2'>Linear Discriminant Analysis</font>(LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeeec6c",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch08/homl08-15.png\" width=\"400\"/></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
